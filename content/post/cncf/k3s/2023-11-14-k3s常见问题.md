---
layout:     post 
slug:   "k3s-qa"
title:      "k3s常见问题"
subtitle:   ""
description: ""  
date:       2023-03-29
author:     "梁远鹏"
image: "/img/banner-pexels.jpg"
published: true
tags: 
    - k3s
    - cncf
    - kubernetes
categories: 
    - kubernetes
---

# 前言

# 备份恢复

如果是使用默认存储的k3s,也就是使用 sqlite 作为存储,那么 k3s 集群的备份和恢复很简单,只需要备份 sqlite 的 db 文件. 至于使用 local 持久化在本地的数据

目前我在我的测试场景中频繁使用这个能力,将备份好的 K3S 在每一次实验时恢复出一个对应状态的集群,备份内容我通过 oras 将数据备份到的容器镜像里面.

这样我就可以每次都直接创建一个研究的 kubernetes 集群了,例如在没有备份恢复的情况下,我总是需要 部署一个默认的 K3S,然后部署我的研究项目,例如 karmada,接着需要等待 karmada 部署完成,这需要一定的时间,而有了备份恢复之后我就可以直接创建一个带有部署好 karmada 的 kubernetes,节省了很多时间.

在通过外部 Etcd 的场景下,使用 Etcd 的备份数据启动一个新的 K3S 的时候报错了: `bootstrap data already found and encrypted with different token`,原因是 K3S 存储了一个 Token 在数据库中,在 Etcd 下则是存储在了 `/bootstrap/` 路径下,只需要删除对应的数据然后再启动 K3S 就可以正常启动了.

# 集群升级

lank8s.cn 稳定运行在 K3S 之上几年了,这周(20240307)对 K3S 做了一次升级,比想象中的要轻松,当然这也是因为是但节点的缘故.

K3S 的升级分两种,一种是手动升级,另一种是自动化升级,我采用的是自动化升级.

## 第一步,安装控制器

```shell
kubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/latest/download/system-upgrade-controller.yaml
```

## 第二步,编写并应用升级计划

```yaml
# Server plan
apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: server-plan
  namespace: system-upgrade
spec:
  concurrency: 1
  cordon: true
  nodeSelector:
    matchExpressions:
    - key: node-role.kubernetes.io/control-plane
      operator: In
      values:
      - "true"
  serviceAccountName: system-upgrade
  upgrade:
    image: rancher/k3s-upgrade
  version: v1.28.7+k3s1
```

正常来说还需要 apply 一个资源来升级 k3s agent 节点,但我是单节点,因此只需要一个 k3s server 的资源文件就可以了.

我是升级了多次,因此也 apply 了多次 server plan 的资源,从 1.26->1.27->1.28->1.29.

运气比较好,没有遇到例如某个资源被删除了这样的事情.