---
layout:     post 
slug:      "new-container-image-registry-from-cncf"
title:      "新的容器镜像仓库选择"
subtitle:   ""
description: ""
date:       2023-01-18
author:     "梁远鹏"
image: "img/post-bg-2015.jpg"
published: true
tags:
    - zot 
    - cncf
categories: [ CloudNative ]
---

# 简介

Zot 是思科开源的遵循 OCI 规范的容器镜像仓库,目前捐赠给了 CNCF,是 Sandbox 级别项目.

本文主要讲述作为镜像代理仓库下的应用场景.

# 部署

## Kubernetes

### 前提

- helm
- kubernetes cluster

这里使用 kind 来创建一个研究用的 K8S 单节点集群.

TODO 
配置了 gcr.io 和 k8s.gcr.io 和 registry.k8s.io 镜像加速的 kind 配置.
配置一个固定的端口映射,用于访问 kind k8s 中的 zot


```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
  apiServerAddress: "0.0.0.0"
  apiServerPort: 6443
containerdConfigPatches:
- |-
  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."gcr.io"]
    endpoint = ["https://gcr.lank8s.cn"]
  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."k8s.gcr.io"]
    endpoint = ["https://k8s.lank8s.cn"]
  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."registry.k8s.io"]
    endpoint = ["https://registry.lank8s.cn"]
nodes:
- role: control-plane
  extraMounts:
  - hostPath: /var/log/tmp
    containerPath: /var/log
  extraPortMappings:
  - containerPort: 5000
    hostPort: 31150
    listenAddress: "0.0.0.0"
```

本文使用的版本是

k8s 1.25.2

```shell
CHART           APP VERSION
zot-0.1.18      v2.0.0-rc2 
```

### 开始部署

这里使用官方的 helm chart 来部署 zot,
```shell
helm repo add project-zot http://zotregistry.io/helm-charts
helm install zot project-zot/zot
```

TODO
配置固定的 nodePort 端口号,用于测试访问.

使用默认的配置来部署zot? 需要配置文件,因为要配置 sync

目前helm还比较初期,使用docker来部署?

# 开启认证

默认情况下,所有用户都可以做任何操作,包括 读取/创建/更新/删除,在镜像代理仓库的场景下,这显然是不合理的,匿名用户只需要具备读权限即可.而管理员则具备其他权限,例如删除操作.

到目前为止 Zot 几种认证:

## 客户端

- TLS 证书认证 (basic TLS 和 mTLS)
- HTTP basic 认证
- HTTP bearer 认证

## 服务器端

- LDAP
- httpasswd

当同时配置了 LDAP 和 httpasswd 时,优先使用 LDAP 认证，如果 LDAP 服务不可用时,回退到 httpasswd 认证机制.

## 实战

本文使用 HTTP basic 认证作为演示效果.

首先需要在 zot 的配置文件中开启 httpasswd 方式的认证:

```json
...
"http": {
...
"auth": {
            "htpasswd": {
              "path": "/tmp/htpasswd"
            }
        }
}
...
```

先创建一个管理员用户, 账号和密码都是 `admin`
```shell
htpasswd -bc /tmp/htpasswd admin admin
```

首先拉取一下镜像,
docker pull localhost:10015/kube-proxy:v1.26.0

尝试用匿名用户来删除镜像:
```shell
curl -X DELETE http://localhost:10015/v2/kube-proxy/manifests/v1.26.0
```

可以看到会提示没权限,添加用户密码后再尝试一次:


```shell
echo -n admin:admin | base64
curl -X DELETE -H "Authorization: Basic: " http://localhost:10015/v2/kube-proxy/manifests/v1.26.0

```

可以看到,请求返回成功了,检查一下镜像数据是否还在:
```shell
```

这时候已经没有镜像数据了,从 Zot 的日志中也可以看到相关删除操作的提示:
```shell
```


# 扩展

  "extensions": {
    "metrics": {},
    "sync": {},
    "search": {},
    "scrub": {},
    "lint": {}
  }


# 实际案例

## lank8s

目前 lank8s 服务已经从 distribution 仓库切换到了 Zot 镜像仓库.对于 lank8s 来说,Zot 的一个比较大的吸引力是可以对多个容器镜像仓库代理,在 Distribution 的 repo 中早有人提了这个功能 issue,但是一直没有人来实现这部分内容.

```json
{
    "extensions": {
        "scrub": {
            "enable": true,
            "interval": "3h"
        },
        "sync": {
            "enable": true,
            "registries": [
                {
                    "urls": [
                        "https://gcr.lank8s.cn"
                    ],
                    "onDemand": true,
                    "tlsVerify": true,
                    "maxRetries": 2,
                    "retryDelay": "5m",
                    "content": [
                    {
                        "prefix": "/google-containers/*"
                    }]
                },
                {
                    "urls": [
                        "https://k8s.lank8s.cn"
                    ],
                    "onDemand": true,
                    "tlsVerify": true,
                    "maxRetries": 2,
                    "retryDelay": "5m"
                },
                {
                    "urls": [
                        "https://registry.lank8s.cn"
                    ],
                    "onDemand": true,
                    "tlsVerify": true,
                    "maxRetries": 2,
                    "retryDelay": "5m"
                }
            ]
        }
    }
}
```

上面的配置示例是当拉取镜像时(例如 kube-proxy:v1.26.0)如果 Zot 本地没有镜像信息则会同时从上游`k8s.lank8s.cn`和`registry.lank8s.cn`,
拉取,为什么没有从另一个上游`gcr.lank8s.cn`拉取镜像呢?是因为在字段`content`中配置了`prefix`,只有前缀为 `google-containers` 的容器镜像才会从上游 `gcr.lank8s.cn` 拉取镜像.

## kind local registry

目前我在使用 kind 创建 k8s 集群做一些实验时,也是使用了 zot 作为实验时拉取到的容器镜像的持久化管理,避免每次用 kind 创建一个新的 k8s 测试集群环境时总是要拉取一次镜像.

https://kind.sigs.k8s.io/docs/user/local-registry/

## 初体验
### 用户不友好

下面列出几个不太友好的地方.

- 当从上游同步镜像时是阻塞整个镜像下载过程的,需要等待 Zot 将镜像从上游整个下载到本地之后,才会让客户端继续镜像的下载.而 Distribution 是分步同步的,因此可以很快的给到客户端响应,用户体验会相对较好.

- 作为镜像代理模式工作时不支持S3存储.

- 同步多个上游镜像源时无法分开存储,例如上游 1 的数据存储在路径 A,上游 2 的数据存储在路径 B. 有一个subpath的功能,不过这个针对的是 namespace 级别的.

### 无法直接使用 docker push

当然这不代表有问题,因为 Zot 项目的设计原则就是 OCI 镜像仓库,不实现任何供应商相关的协议,当然也包括Docker.

# 注意 

本文还在持续创作当中.