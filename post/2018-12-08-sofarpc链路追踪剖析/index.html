<!doctype html><html lang=en-us><head><head><meta name=google-site-verification content="9vIieCe-Qpd78QOmBl63rGtIVbhY6sYyuxX3j8XWBA4"><meta name=baidu-site-verification content="LRrmH41lz7"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=google-site-verification content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=baidu-site-verification content="HGLXRsUXC4"><meta name=description content="微服务已经被广泛应用在工业界，微服务带来易于团队并行开发、独立部署、模块化管理等诸多优点。然而微服务将原单体拆分多个模块独立部署，各模块之间链接变得错综复杂，在大规模分布式系统中这种复杂链路给维护带来了诸多困难。如果对整个微服务架构不能了然于胸，便很难理清各模块之间的调用关系。 例如修改一个服务接口，对哪些服务造成影响不能快速定位。"><meta name=keyword content="liangyuanpeng , lan , lyp的网络日志, lan的博客, lan Blog, 博客, 个人网站, 互联网, Web,  Istio, 微服务"><link rel="shortcut icon" href=img/favicon.ico><title>转|SOFARPC链路追踪剖析-lyp的博客 | lyp Blog</title><link rel=canonical href=/post/2018-12-08-sofarpc%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA%E5%89%96%E6%9E%90/><link rel=stylesheet href=https://res.cloudinary.com/lyp/raw/upload/v1537369740/hugo/css/iDisqus.min.css><link rel=stylesheet href=https://res.cloudinary.com/lyp/raw/upload/v1537369744/hugo/css/bootstrap.min.css><link rel=stylesheet href=https://res.cloudinary.com/lyp/raw/upload/v1537369740/hugo/css/hux-blog.min.css><link rel=stylesheet href=https://res.cloudinary.com/lyp/raw/upload/v1537369740/hugo/css/syntax.css><link href=//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css rel=stylesheet type=text/css><script src=https://res.cloudinary.com/lyp/raw/upload/v1537369966/hugo/js/jquery.min.js></script><script src=https://res.cloudinary.com/lyp/raw/upload/v1537369966/hugo/js/bootstrap.min.js></script><script src=https://res.cloudinary.com/lyp/raw/upload/v1537369964/hugo/js/hux-blog.min.js></script></head></head><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span><span class=icon-bar></span><span class=icon-bar></span></button>
<a class=navbar-brand href=/>Hi,I`m lan</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>Home</a></li><li><a href=categories/cloudnative>cloudnative</a></li><li><a href=categories/devops>devops</a></li><li><a href=categories/kubernetes>kubernetes</a></li><li><a href=categories/tech>tech</a></li><li><a href=categories/%E4%B8%AD%E9%97%B4%E4%BB%B6>中间件</a></li><li><a href=categories/%E5%AE%B9%E5%99%A8>容器</a></li><li><a href=categories/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92>容器编排</a></li><li><a href=categories/%E7%89%A9%E8%81%94%E7%BD%91>物联网</a></li><li><a href=search>SEARCH <img src=img/search.png height=15 style=cursor:pointer></a></li></ul></div></div></div></nav><script>var $body=document.body;var $toggle=document.querySelector('.navbar-toggle');var $navbar=document.querySelector('#huxblog_navbar');var $collapse=document.querySelector('.navbar-collapse');$toggle.addEventListener('click',handleMagic)
function handleMagic(e){if($navbar.className.indexOf('in')>0){$navbar.className=" ";setTimeout(function(){if($navbar.className.indexOf('in')<0){$collapse.style.height="0px"}},400)}else{$collapse.style.height="auto"
$navbar.className+=" in";}}</script><style type=text/css>header.intro-header{background-image:url(https://res.cloudinary.com/lyp/image/upload/v1544363182/hugo/blog.github.io/e64ae3596ed565b8202d395d771665dd.jpg)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/rpc title=rpc>rpc</a>
<a class=tag href=/tags/sofa title=sofa>sofa</a>
<a class=tag href=/tags/sofastack title=sofastack>sofastack</a>
<a class=tag href=/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6 title=中间件>中间件</a></div><h1>转|SOFARPC链路追踪剖析</h1><h2 class=subheading></h2><span class=meta>Posted by lyp on Saturday, December 8, 2018
<span id=busuanzi_container_page_pv>|<span id=busuanzi_value_page_pv></span><span>
<span id=/post/2018-12-08-sofarpc%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA%E5%89%96%E6%9E%90/ class="leancloud_visitors meta_data_item" data-flag-title><span class=post-meta-item-icon><span class="octicon octicon-eye"></span></span><i class="fa fa-eye"></i><span class=old-visitors-count style=display:none></span><span class=leancloud-visitors-count></span></span><script src=https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js></script><script>AV.initialize("","");</script><script type=text/javascript>function showTime(Counter){var query=new AV.Query(Counter);var entries=[];var $visitors=$(".leancloud_visitors");$visitors.each(function(){entries.push($(this).attr("id").trim());});query.containedIn('url',entries);query.find().done(function(results){var COUNT_CONTAINER_REF='.leancloud-visitors-count';var OLD_COUNT_CONTAINER_REF='.old-visitors-count';for(var i=0;i<results.length;i++){var item=results[i];var url=item.get('url');var time=item.get('time');var element=document.getElementById(url);$(element).find(COUNT_CONTAINER_REF).text(time);}
for(var i=0;i<entries.length;i++){var url=entries[i];var element=document.getElementById(url);var countSpan=$(element).find(COUNT_CONTAINER_REF);if(countSpan.text()==''){var oldCountSpan=$(element).find(OLD_COUNT_CONTAINER_REF).text();if(oldCountSpan!=''){countSpan.text(0+parseInt(oldCountSpan));}else{countSpan.text(0);}}}}).fail(function(object,error){console.log("Error: "+error.code+" "+error.message);});}
function addCount(Counter){var $visitors=$(".leancloud_visitors");var url=$visitors.attr('id').trim();var title=$visitors.attr('data-flag-title').trim();var query=new AV.Query(Counter);query.equalTo("url",url);query.find({success:function(results){if(results.length>0){var counter=results[0];counter.fetchWhenSave(true);counter.increment("time");counter.save(null,{success:function(counter){var $element=$(document.getElementById(url));$element.find('.leancloud-visitors-count').text(counter.get('time'));},error:function(counter,error){console.log('Failed to save Visitor num, with error message: '+error.message);}});}else{var newcounter=new Counter();var acl=new AV.ACL();acl.setPublicReadAccess(true);acl.setPublicWriteAccess(true);newcounter.setACL(acl);newcounter.set("title",title);newcounter.set("url",url);var OLD_COUNT_CONTAINER_REF='.old-visitors-count';var $element=$(document.getElementById(url));var oldCountSpan=$element.find(OLD_COUNT_CONTAINER_REF).text();if(oldCountSpan!=''){newcounter.set("time",parseInt(oldCountSpan)+1);}else{newcounter.set("time",1);}
newcounter.save(null,{success:function(newcounter){var $element=$(document.getElementById(url));$element.find('.leancloud-visitors-count').text(newcounter.get('time'));},error:function(newcounter,error){console.log('Failed to create');}});}},error:function(error){console.log('Error:'+error.code+" "+error.message);}});}
$(function(){var Counter=AV.Object.extend("Counter");if($('.leancloud_visitors').length==1){addCount(Counter);}else{showTime(Counter);}});</script>阅读 </span></span>|<span class=post-date>共8032字</span>，阅读约<span class=more-meta> 17 分钟</span></span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><header><h2>TOC</h2></header><nav id=TableOfContents><ul><li><a href=#31-可插拔设计>3.1 可插拔设计</a></li><li><a href=#32-事件总线设计>3.2 事件总线设计</a></li><li><a href=#33-调用链trace-和span>3.3 调用链Trace 和Span</a><ul><li><a href=#331-traceid-生成规则>3.3.1 TraceId 生成规则</a></li><li><a href=#332-spanid-生成规则>3.3.2 SpanId 生成规则</a></li></ul></li><li><a href=#34-数据采样设计>3.4 数据采样设计</a></li><li><a href=#35-异步刷新机制>3.5 异步刷新机制</a></li><li><a href=#36-耗时计算>3.6 耗时计算</a></li><li><a href=#37-埋点数据透传>3.7 埋点数据透传</a></li><li><a href=#38-异步线程的链路调动>3.8 异步线程的链路调动</a></li><li><a href=#39-文件存储结构>3.9 文件存储结构</a></li></ul><ul><li><a href=#41-dapper>4.1 Dapper</a></li><li><a href=#42-鹰眼>4.2 鹰眼</a></li><li><a href=#43-cat>4.3 CAT</a></li><li><a href=#44-skywalking>4.4 Skywalking</a></li><li><a href=#45-zipkin>4.5 zipkin</a></li><li><a href=#46-span-data-typecolor-stylecolorrgb0-0-0span-data-typebackground-stylebackground-colorrgb255-255-255spring-cloud-sleuthspanspan>4.6 Spring Cloud Sleuth</a></li><li><a href=#47-对比总结>4.7 对比总结</a></li></ul></nav><h1 id=一-前言>一. 前言</h1><p>微服务已经被广泛应用在工业界，微服务带来易于团队并行开发、独立部署、模块化管理等诸多优点。然而微服务将原单体拆分多个模块独立部署，各模块之间链接变得错综复杂，在大规模分布式系统中这种复杂链路给维护带来了诸多困难。 如果对整个微服务架构不能了然于胸，便很难理清各模块之间的调用关系。 例如修改一个服务接口，对哪些服务造成影响不能快速定位。</p><p>SOFARPC 在5.4.0 以后提供了链路追踪技术，可以有效协助开发运营人员进行故障诊断、容量预估、性能瓶颈定位以及调用链路梳理。</p><p>如思维导图所示，本文将从以下几个方面介绍目前已经开源的 SOFARPC 的链路追踪技术：</p><p>以大规模分布式电商系统为例，用户下单购买某款产品时后端需要调用各系统或子模块进行协作，共同完成一个用户请求。 如下图所示，用户的下单行为涉及到了A、B、C、D、E、F 6个系统的协同工作，这些系统都是以集群形式部署。整个链路中最长的链路调用是3层，如 A-> C -> E 或 A -> C -> F。</p><p><img src=https://res.cloudinary.com/lyp/image/upload/v1544256847/hugo/blog.github.io/sofa-rpc/1533886847606-638e0531-a016-445f-9d94-ecb6bc5531d1.png alt="image.png | left | 652x251"></p><p>模块的增多加大了系统出错的概率，一旦因某系统/模块出错导致整个请求调用出错，在缺乏链路追踪的条件下很难定位具体出错的模块，只能通过日志搜索定位。 在实际生产环境下比较关注一个请求中的各个模块的耗时情况、连续调用情况、出错的节点等信息。  </p><p>为了解决上述问题，Dapper提供了一套解决方案。整个方案分为数据收集、存储和分析几个部分。分布式追踪技术会__记录__一个请求中各模块的调用信息；并通过一个处理集群把所有机器上的日志增量地收集到集群当中进行处理，将同一个请求的日志串联；最后可视化显示调用情况。</p><p>常用的数据收集方式为埋点，通过在公共组件如RPC等注入代码，收集服务调用的相关信息。目前大部分链路调用系统如Dapper、鹰眼、Spring Cloud Sleuth 都在用这种模式。同样SOFARPC 作为一个公共的通讯框架，在金融业务领域被广泛应用，因此适合作为埋点，这样无需业务端自行埋点，可做到对业务代码的无侵入性。</p><p>Dapper 将一个调用过程构建成一棵调用树(称为Tracer)，Tracer树中的每个节点表示链路调用中的一个模块或系统。 通过一个全局唯一的 traceId 来标识一个请求调用链。 并定义了span，span表示一个系统的调用，一个span 包含以下阶段：</p><ol><li>Start:  发起调用</li><li>cleint send(cs): 客户端发送请求</li><li>Server Recv(sr)：服务端收到请求</li><li>Server Send(ss): 服务端发送响应</li><li>Client Recv(cr) : 客户端收到服务端响应</li><li>End： 整个链路完成</li></ol><p><img src=https://res.cloudinary.com/lyp/image/upload/v1544256865/hugo/blog.github.io/sofa-rpc/1533888396053-61f12b7f-926b-4c44-958c-e648133bfe32.png alt="image.png | left | 747x320"></p><p>  （图来自Dapper论文）</p><p>每个span 包含两个重要的信息 span id(当前模块的span id) 和 span parent ID(上一个调用模块的span id)，通过这两个信息可以定位一个span 在调用链的位置。 通过以上信息我们可以定义用户下单过程的调用链：</p><p><img src=https://res.cloudinary.com/lyp/image/upload/v1544256881/hugo/blog.github.io/sofa-rpc/1533891501586-643c5f31-ecea-4b64-84c6-c0e2f42a8bc7.png alt="image.png | left | 696x336"></p><p>SOFARPC中的链路追踪技术主要是作为埋点部分，因此对于链路追踪系统的收集和分析部分本文不做详述，想要深入了解的可参看参考资料内容。链路追踪可以提供我们以下功能:</p><ol><li><strong>服务耗时、瓶颈分析</strong> ：分析每个服务的耗时情况，可以针对耗时长的服务进行优化，提高服务性能。</li><li><strong>可视化错误</strong>：快速定位服务链路中出错的环境，便于排查和定位问题。一般链路追踪中间件都提供了ZipKin 页面支持。</li><li><strong>链路优化</strong>: 对于调用比较频繁的服务，可以针对这些服务实施一些优化措施。</li><li><strong>调用链路梳理</strong>：通过可视化界面，对整个调用链路有个清晰的认识。</li></ol><p>在设计分布式链路框架时需要考虑一下几个问题：</p><ol><li><strong>低损耗、高性能</strong>: 追踪系统对在线服务的影响应该做到足够小，不影响线上服务性能。</li><li><strong>应用透明</strong>: 对于业务开发人员来说，应不需要知道有跟踪系统这回事的。</li><li><strong>可扩展性</strong>：虽则业务规则增大、集群增多，监控系统都应该能完全把控住这种快速变化。</li><li>__数据采样设计：__如果每条日志都记录，在高并发情况下对系统有一定的损耗。但收集数据太少可能对统计结果有所影响，所以应合理设计采样比例。</li></ol><h1 id=三-sofarpc-链路追踪设计原理>三. SOFARPC 链路追踪设计原理</h1><p>SOFARPC 作为一个基础的通讯中间件，对服务调用有很强的感知能力，容易获取链路追踪所需的服务调用信息。因此很多链路追踪系统都会选择RPC 作为埋点对象，通过对RPC中间件的埋点可以轻松做到对用户的无感知、透明化。 SOFARPC在5.4.0 以后开始支持分布式链路追踪，其技术实现主要依赖于所集成的SOFATracer。</p><p>SOFARPC 不仅提供了埋点信息采集的能力, 还支持数据上报zipkin。 通过SOFARPC + SOFATracer + zipKin 可以快速搭建一套完整的链路追踪系统，包括埋点、收集、分析展示等。 收集和分析主要是借用zipKin的能力，本文重点讲SOFARPC中的数据埋点设计。SOFARPC自身具备的微内核设计和可拓展性，使得在SOFARPC在不破坏开放封闭原则的前提下，比较轻松地集合SOFATracer。该部分主要从以下几个方面讨论SOFARPC的链路追踪设计思路：</p><ol><li>可插拔设计。 SOFARPC采用了微内核设计，使得很容易扩展，增加新功能。</li><li>总线设计。为数据埋点做提供一种无侵入的扩展方式。</li><li>调用trace和span</li><li>数据采样设计</li><li>异步刷新机制</li><li>耗时计算：链路调用的耗时统计等信息获取。</li><li>埋点数据透传，各模块之间的链路调用数据的透传机制。</li><li>异步线程的链路调用。在异步多线程环境下如何保证traceId和 spanId的有序性。</li><li>链路调用日志数据的文件存储结构</li></ol><h2 id=31-可插拔设计>3.1 可插拔设计</h2><p>SOFARPC自身具备的微内核设计和可拓展性，使得在SOFARPC在不破坏开放封闭原则的前提下，比较轻松地集合SOFATracer。SOFARPC 采用了自己实现的一套SPI机制， 通过该SPI机制动态去加载其他模块、过滤器、协议等，实现灵活拓展。SOFARPC 为了集成SOFATracer也采用了这套机制，做到可插拔。</p><p><img src=https://res.cloudinary.com/lyp/image/upload/v1544256900/hugo/blog.github.io/sofa-rpc/1533899265171-4cb78a65-7f37-4b38-9579-3f5a7503f7e9.png alt="image.png | left | 275x264"></p><p>SofaTracerModule 类实现了Module 接口，并增加 @Extension(&ldquo;sofaTracer&rdquo;) 注解，方便SOFARPC在启动时将相关模块加载进来。 SofaTracerModule 作为SOFA-PRC 链路追踪的入口，在SofaTracerModule模块被加载时完成一些事件的订阅。</p><p>这里会订阅 9 种事件， 通过监听SOFARPC的这 9 种事件，来完成埋点数据的获取和异步磁盘写入操作。SOFARPC通过事件总线设计来订阅这些事件，当事件发生时通知对应的订阅者做相应的操作。</p><h2 id=32-事件总线设计>3.2 事件总线设计</h2><p>事件总线(EventBus)设计也是 SOFARPC的一个具有很强扩展性的设计，EventBus 类似计算机数据总线，用于传输数据，EventBus主要是传输事件数据。 EventBus 采用了发布-订阅设计模式，在SOFARPC 服务调用的整个过程中设置多个事件点，当这些事件发生时就创建事件写入到EventBus，订阅者可以订阅总线中感兴趣的事件并处理。如图所示：</p><p><img src=https://res.cloudinary.com/lyp/image/upload/v1544256925/hugo/blog.github.io/sofa-rpc/1533902429983-43952663-4097-4594-bace-311db5f5df7b.png alt="image.png | left | 723x433"></p><p>如上图所示，SOFATracer 订阅了在RPC调用周期的9种事件，当这些事件发生时会创建事件传入到EventBus。 EventBus中一旦发布新的事件就会通知所有感兴趣的订阅者，SAFA-Tracer 统一采用 SofaTracerSubscriber 订阅和处理这9种事件，最终链路追踪数据的获取操作都交给了RpcSofaTracer处理。</p><p>这种总线设计使得SOFARPC在集合SOFATracer时无需为了获取数据而破坏原来代码的封装性，使用无侵入的方式来完成埋点和数据的获取。</p><h2 id=33-调用链trace-和span>3.3 调用链Trace 和Span</h2><p>SOFATracer的设计思路也是来自Dapper, 因此也提供了调用树Trace 和 Span。</p><ul><li>Trace是一次完整的跟踪，从请求到服务器开始，服务器返回response结束，跟踪每次rpc调用的耗时。 Trace是一个类似树状调用链，树中的每个节点对应一个系统或服务节点。</li><li>Span是一个更小的单位，表示一个RPC调用过程。在SOFARPC中分为 ClientSpan 和ServerSpan。 ClientSpan记录从客户端发送请求给服务端，到接受到服务端响应结果的过程。ServerSpan是服务端收到客户端时间 到 发送响应结果给客户端的这段过程。</li></ul><p>一个span 包含几种Annotation:</p><ol><li>cs (client send)</li><li>cr (client recv)</li><li>sr (server recv)</li><li>ss (server send)</li></ol><p><img src=https://res.cloudinary.com/lyp/image/upload/v1544256947/hugo/blog.github.io/sofa-rpc/1533904836651-b07e9fac-c825-4597-9241-00071deb12f7.png alt="image.png | left | 452x183"></p><p>如上图所示展示了两个系统调用中的client span 和server span的关系， 一次RPC调用称为span, 并产生一个spanId, client span 的spanId 和 server span的spanId是同一个，因为都在一个RPC调用中。 下图展示从时间的维度来解释这两者的关系：</p><p><img src=https://res.cloudinary.com/lyp/image/upload/v1544256966/hugo/blog.github.io/sofa-rpc/1533905092654-7d6f2ccc-5670-486b-9b27-8571ffc41345.png alt="image.png | left | 505x158"></p><h3 id=331-traceid-生成规则>3.3.1 TraceId 生成规则</h3><p>TraceId 指的是 SOFATracer 中代表唯一一次请求的 ID，此 ID 一般由集群中第一个处理请求的系统产生，并在分布式调用下通过网络传递到下一个被请求系统。</p><p>traceId应当保证全局唯一，如何做到全局唯一呢？TraceId 目前的生成的规则参考了淘宝的鹰眼组件：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plain data-lang=plain>服务器 IP + 产生 ID 时候的时间 + 自增序列 + 当前进程号 
</code></pre></div><p>如下图所示：</p><p><img src=https://res.cloudinary.com/lyp/image/upload/v1544256985/hugo/blog.github.io/sofa-rpc/1533905964000-3da0de78-3ee0-4c7c-9b47-d83629b6467e.png alt="image.png | left | 474x92"></p><ul><li>自增的序列，从 1000 涨到 9000，到达 9000 后回到 1000 再开始往上涨。
根据这种方式计算出的 traceId 为 0ad1348f1403169275002100356696，可以保证 traceId 全局唯一。</li></ul><h3 id=332-spanid-生成规则>3.3.2 SpanId 生成规则</h3><p>SpanId 表示整个链路中一次rpc调用的序号，也代表了本次请求在整个调用链路中的位置或者说层次，比如 A 系统在处理一个请求的过程中依次调用了 B，C，D 三个系统，那么这三次调用的的 SpanId 分别是：0.1，0.2，0.3。如果 C 系统继续调用了 E，F 两个系统，那么这两次调用的 SpanId 分别是：0.2.1，0.2.2。</p><p>SOFARPC 和 Dapper不同，spanId中已经包含了调用链上下文关系，包含parent spanId 的信息。如图所示：</p><p><img src=https://res.cloudinary.com/lyp/image/upload/v1544257006/hugo/blog.github.io/sofa-rpc/1533906416932-fc74669f-9b1d-43a3-a544-f5d8318502d8.png alt="image.png | left | 596x282"></p><h2 id=34-数据采样设计>3.4 数据采样设计</h2><p>在Dapper 论文中强调了数据采样的重要性，如果将每条埋点数据都刷新到磁盘上会增大链路追踪框架对原有业务性能的影响。如果采样率太低，可能会导致一些重要数据的丢失。 论文中提到如果在高并发情况下 1/1024 的采样率是足够的，也不必担心重要事件数据的丢失。因为在高并发环境下，一个异常数据出现一次，那么就会出现1000次。 然而在并发量不是很多的系统，并且对数据各位敏感时需要让业务开发人员手动设置采样率。</p><p>SOFATracer 的不支持配置 采样率，但采样率也不是一个固定写死的值，而是采用自适应采样率。 在SOFATracer 中提供了RingBuffer的数据结构，设置一个 1024 的序列化槽位用于存储每个链路调用的埋点数据。 可以看做是一个圆形环状结构，RingBuffer 中的数据 从槽位0 开始存储，一直存储到1023。 当操作1023时会从头开始存放，放在原来槽位的数据将被覆盖。</p><p><img src=https://res.cloudinary.com/lyp/image/upload/v1544257112/hugo/blog.github.io/sofa-rpc/1533912549140-db8fa96d-82bd-489d-9430-d5adb0a7978b.png alt="image.png | left | 450x293"></p><p>从上图所示，当数据写入的速度远远大于 3个consumer的处理速度，那么环上的数据在未被处理时就被覆盖。 通过覆盖的方式来自动调整采样率，并发性越高、写入速度越快时，采样率就越低。当并发性越低、写入速度也随之变慢，则采样率就变高。</p><p>在SOFATracer中默认开启三个线程去负责这些数据的持久化。假设每个线程的处理速度是 x/s(条/秒)， 并发写入的速度是 y/s，那么SOFATracer的自动化采样率为 3x / y (前提是 y >= 3x, 否则就是 100%采样率)。</p><h2 id=35-异步刷新机制>3.5 异步刷新机制</h2><p>埋点数据的本地化存储涉及到磁盘操作， 磁盘IO速度较慢，如果在高并发环境下同步刷新磁盘给原业务带来的性能损耗是非常可观的。 链路追踪系统在数据埋点的时候应尽可能的降低系统损耗，对原业务在逻辑和性能上做到无侵入性。</p><p>SOFATracer 采用了异步刷新机制，将RingBuffer的数据异步刷新到磁盘。 默认情况下 会启动 3个处理线程去处理 RingBuffer 的数据，将数据异步刷新到磁盘进行持久化。</p><p>当RingBuffer 写入新数据时就会唤醒处理线程， 并将当前存入数据的槽位设置为可用槽位。 处理线程从睡眠点醒来后，便从原来处理位置往下获取数据并处理，直到所处理数据槽位大于可用槽位，则阻塞等待。</p><h2 id=36-耗时计算>3.6 耗时计算</h2><p>耗时计算不是为了集成SOFATracer 而单独设置的，SOFARPC 框架自身带有耗时计算的逻辑，这些时间可以用于判断RPC调用是否超时等。因此加入链路追踪埋点时，不需要在扩展模块中计算耗时时间，SOFARPC中已经将调用的时间耗时等信息放在 RpcInternalContext 上下文中。 在计算RPC调用耗时时，对原有框架性能不影响，直接去上下文获取即可。</p><h2 id=37-埋点数据透传>3.7 埋点数据透传</h2><p>各模块部署后独立收集埋点日志，这些调用链日志通过traceId串联在一起。 在SOFARPC中，下一个模块的spanId的创建依赖于上一个模块的spanId。 因此这些埋点数据如traceId以及spanId需要透传给下游模块。</p><p>数据传输一般有两种：</p><ol><li>带内透传，即在原来的rpc 调用请求网络宽带中加入埋点数据透传给下游；</li><li>带外传播，通过单独提供一个宽带来传播，不影响原调用数据和网络。</li></ol><p>Dapper 采用带外传播，这种方式可以不影响原有业务性能。带内透传数据意味着需要增加原来网络调用的负载。SOFARPC 采用的是带内透传，直接在原来的RPCRequest的扩展字段中加入埋点数据，直接透传给下游。SOFARPC的spanId长度相对较短，所需传递的数据相对较小，从整体上看对原业务性能影响较小。</p><h2 id=38-异步线程的链路调动>3.8 异步线程的链路调动</h2><p>在多线程并发调用环境下的数据链路埋点也是一个值得关注问题，当一个服务考虑性能问题可能会起多个线程同时调用其他不同的模块。链路系统如何保证这些调用还是符合 openTrace 规范，保证traceId 和 spanId 有序。</p><p><img src=https://res.cloudinary.com/lyp/image/upload/v1544257132/hugo/blog.github.io/sofa-rpc/1533949332771-775bc5df-48cd-40ba-975c-077f684584c7.png alt="image.png | left | 428x170"></p><p>一个链路调用在模块A 是一个线程，链路调用的上下文信息如traceId、spanId等都是存放在ThreadLocal。 按上图思路新起的线程1、2、3无法获取主线程的ThreadLocal数据，即无法获取调用链路数据。那么在无法获取链路调用的上线文数据时进行模块B、C、D的调用操作会导致收集得到的埋点数据是乱序的脏数据。</p><p>为了避免启动新线程把 链路调用的上下文 信息丢失，SOFATracer 提供了SofaTracerCallable类，只要使用该类来实现线程逻辑，SOFATracer会自动将链路调用的上下文信息透传给 SofaTracerCallable，因此可以像单线程一样进行调用埋点。SOFATracer 将上下文中的一些字段设置为线程安全，同样保证了多线程环境下的数据安全问题。 因此建议在多线程环境下进行一步调用时尽可能考虑使用 SofaTracerCallable， 否则调用链数据与预期有些出路。</p><h2 id=39-文件存储结构>3.9 文件存储结构</h2><p>SOFA 整体开源框架对日志做了很好地分类，将不同类型的日志存放在不同的文件夹下。一方面便于收集特定日志，如埋点数据；另一方面也便于查找问题方便，日志结构和内容清晰。</p><p>在SOFARPC的链路追踪技术中，埋点数据的存储也采用日志文件方式进行持久化存储。tracer日志文件包含以下文件：</p><table><thead><tr><th>文件</th><th>功能</th></tr></thead><tbody><tr><td>rpc-client-digest.log</td><td>记录client rpc 调用的链路调用数据</td></tr><tr><td>rpc-client-stat.log</td><td>记录 client rpc 链路调用的统计数据</td></tr><tr><td>rpc-server-digest.log</td><td>记录 server rpc 调用的链路调用数据</td></tr><tr><td>rpc-server-stat.log</td><td>记录 server rpc 链路调用的统计数据</td></tr><tr><td>static-info.log</td><td>统计信息日志</td></tr><tr><td>tracer-self.log</td><td>tracer 自身的日志记录</td></tr></tbody></table><h1 id=四与其他链路追踪框架对比>四.与其他链路追踪框架对比</h1><h2 id=41-dapper>4.1 Dapper</h2><p>Dapper是Google生产环境下的分布式跟踪系统，google也基于该系统发布了一篇著名的链路追踪设计论文<a href="https://www.researchgate.net/profile/Luiz_Barroso/publication/239595848_Dapper_a_Large-Scale_Distributed_Systems_Tracing_Infrastructure/links/5474acdc0cf29afed60f9031/Dapper-a-Large-Scale-Distributed-Systems-Tracing-Infrastructure.pdf?origin=publication_detail">《Dapper, a Large-Scale Distributed Systems Tracing Infrastructure》</a>。 Dapper是未开源项目，现大部分分布式链路追踪系统都是借鉴该论文思路进行设计。Dapper有三个设计目标：</p><ul><li>低消耗：跟踪系统对在线服务的影响应该做到足够小。</li><li>应用级的透明：对于应用的程序员来说，是不需要知道有跟踪系统这回事的。如果一个跟踪系统想生效，就必须需要依赖应用的开发者主动配合，那么这个跟踪系统显然是侵入性太强的。</li><li>延展性：Google至少在未来几年的服务和集群的规模，监控系统都应该能完全把控住</li></ul><p><img src=https://res.cloudinary.com/lyp/image/upload/v1544257150/hugo/blog.github.io/sofa-rpc/1533967069006-4f78fa15-2e9d-4505-864d-dfd468ac7e89.png alt="image.png | left | 486x313"></p><p>（图来自Dapper论文）</p><p>如上图所示，Dapper的设计可以分为三个步骤：</p><ol><li>无侵入化埋点。Dapper通过对少量的公共组件进行改造，可以以对应用开发者近乎零浸入的成本对分布式控制路径进行跟踪。</li><li>后台进程收集日志。Dapper的守护进程和收集组件把这些数据从生产环境的主机中拉出来。</li><li>数据存储。收集后的数据存储到Bigtable。</li></ol><h2 id=42-鹰眼>4.2 鹰眼</h2><p>鹰眼是来自淘宝的链路追踪系统，主要借鉴了Dapper的论文思路，鹰眼系统具有一整套完整的数据埋点、收集、存储和分析方案。 和Dapper相似，其设计思路如下：</p><ol><li>基于公共中间件完成数据的埋点和日志生成</li><li>采用实时数据抓取，使用一个额外的后台进程定期（时间间隔小）收集日志。需要在每台业务server上都部署并管理日志收集agent，在机器资源不足时对业务系统性能有一定影响。</li><li>并结合实时+离线存储的设计思路，将数据存放在HDFS和Hbase中</li><li>按traceId汇总调用链，按RpcId重组调用链信息</li><li>采用流式计算引擎分析和统计调用链</li></ol><p><img src=https://res.cloudinary.com/lyp/image/upload/v1544257170/hugo/blog.github.io/sofa-rpc/1533966874882-8402c527-a4e3-41c3-9698-ed7142c1d505.png alt="image.png | left | 524x314"></p><p>  （图来自鹰眼分享PPT）</p><h2 id=43-cat>4.3 CAT</h2><p>CAT是大众点评的实时应用监控平台，提供了全面的监控服务和业务决策支持。 业务端接入时对原业务代码具有侵入性，需要在业务代码中做埋点处理。直接向日志收集器发异步请求，一台客户端会连向几个服务端，避免数据丢失。采用全量采样，系统繁忙的时候对性能影响较大。</p><p><img src=https://res.cloudinary.com/lyp/image/upload/v1544257189/hugo/blog.github.io/sofa-rpc/1533967370137-b4b61536-ddb2-4618-91f0-c7f7291b7ac4.png alt="image.png | left | 494x253"></p><p>CAT也提供了链路追踪能力，该技术支持分五个阶段：收集、传输、分析、存储和展示。</p><ol><li>收集阶段：业务端自行埋点，通过调用CAT客户端将埋点数据以消息树的形式存入传输队列。</li><li>传输阶段：CAT客户端负责将客户端消息传输到后端，CAT消费机负责接收消息。</li><li>分析阶段：负责报表生成。实时消费调度器会将消费队列消息取出，分发给每个消费者内部队列；报表分析器只会从自己的报表队列中取出消息树，逐个消费，更新报表模型。CAT以小时为单位形成报表，原始日志转储(raw log dump)是一个特殊的分析器，它不生产报表，而是将消息存入本地文件系统。</li><li>存储阶段：负责报表和原始日志的存储，目前报表会存在MySQL中，原始日志压缩后存在HDFS中长久保存。</li><li>展示阶段：负责数据的可视化。作为用户服务入口，负责报表和原始日志的输出显示</li></ol><h2 id=44-skywalking>4.4 Skywalking</h2><p>Skywalking 创建与2015年，提供分布式追踪功能。他被用于追踪、监控和诊断分布式系统，特别是使用微服务架构，云原生或容积技术。提供以下主要功能:分布式追踪和上下文传输、服务性能指标分析、应用拓扑分析、性能优化等。使用java探针字节码增加技术，实现对整个应用的监控，对应用零侵入。</p><p><img src=https://res.cloudinary.com/lyp/image/upload/v1544257210/hugo/blog.github.io/sofa-rpc/1533968855407-80654605-109f-45e6-ab28-d8e199940bc2.png alt="image.png | left | 685x410"></p><p>（图来自Skywalking 官网）</p><p>在使用Skywalking时需要在服务器中安装 agent 负责将数据上传和收集。</p><h2 id=45-zipkin>4.5 zipkin</h2><p>由Twitter团队开源， Zipkin是一个分布式的跟踪系统。它有助于收集数据需要解决潜在的问题在市微服架构的时机。它主要负责管理数据的收集和查找。该产品可以和spring-cloud-sleuth、sofa-trace集合使用，使用较为简单， 集成很方便。</p><h2 id=46-span-data-typecolor-stylecolorrgb0-0-0span-data-typebackground-stylebackground-colorrgb255-255-255spring-cloud-sleuthspanspan>4.6 Spring Cloud Sleuth</h2><p>Spring-Cloud-Sleuth是Spring Cloud的组成部分之一，为SpringCloud应用实现了一种分布式追踪解决方案，其兼容了Zipkin, HTrace和log-based追踪。Spring-Cloud-Sleuth也是采用了Dapper的设计思路，与其他连续系统不同，Spring-Cloud-Sleuth不是一个完整埋点、收集、分析的完整链路追踪系统，Spring-Cloud-Sleuth只是作为链路追踪系统中的埋点环节，类似SOFATracer.</p><h2 id=47-对比总结>4.7 对比总结</h2><h1 id=五-总结>五. 总结</h1><p>SOFARPC的依靠集成SOFATrace来实现链路追踪技术，SOFARPC作为公共组件在整个链路追踪技术系统中负责数据埋点工作。依赖SOFARPC自身强大的可扩展性设计，如微内核设计和事件总线设计，使得SOFARPC在不破坏开发封闭原则的基础上快速实现了链路追踪埋点工作。 SOFARPC的链路追踪技术具有以下特点：</p><ol><li>作为公共基础的通讯组件，SOFARPC的链路追踪埋点对业务代码实现零侵入。</li><li>采用日志数据异步刷新机制，不影响正常业务性能。</li><li>采用了自适应采样设计，巧妙平衡了数据采集和性能的问题。</li><li>支持数据上报zipkin, 通过与zipkin结合可以快速构建一个完整的连续追踪系统。</li><li>解决了异步线程链路调用数据问题。</li><li>采用了<a href=http://opentracing.io/documentation/pages/spec.html>OpenTracing 规范</a>，因此可以和其他链路追踪手机和展示的技术框架快速整合。</li></ol><h1 id=六-参考资料>六. 参考资料</h1><ul><li>[1] <a href="https://www.researchgate.net/profile/Luiz_Barroso/publication/239595848_Dapper_a_Large-Scale_Distributed_Systems_Tracing_Infrastructure/links/5474acdc0cf29afed60f9031/Dapper-a-Large-Scale-Distributed-Systems-Tracing-Infrastructure.pdf?origin=publication_detail">《Dapper, a Large-Scale Distributed Systems Tracing Infrastructure》</a></li><li>[2] <a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2650998224&idx=1&sn=7afed75f23160156b816bfe87966c54f&chksm=bdbefd838ac97495660da0163694625f2c07832ca4d78b4f022609f8c28bdbcbe2170eacfb6c&scene=27#wechat_redirect">全链路稳定性背后的数字化支撑：阿里巴巴鹰眼技术解密</a></li></ul><hr><p>文章转自<a href=http://www.sofastack.tech/post/lgm3af>【剖析 | SOFARPC 框架】之SOFARPC 链路追踪剖析</a></p><hr><ul class=pager><li class=previous><a href=/post/2018-12-05-sofarpc%E6%A1%86%E6%9E%B6%E4%B9%8B%E6%80%BB%E4%BD%93%E8%AE%BE%E8%AE%A1%E4%B8%8E%E6%89%A9%E5%B1%95%E6%9C%BA%E5%88%B6/ data-toggle=tooltip data-placement=top title=转|SOFARPC框架之总体设计与扩展机制>&larr; Previous Post</a></li><li class=next><a href=/post/2018-12-08-awesome%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/ data-toggle=tooltip data-placement=top title=AWESOME消息队列>Next Post &rarr;</a></li></ul><div id=disqus-comment></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/chirpstack title=chirpstack>chirpstack</a>
<a href=/tags/docker title=docker>docker</a>
<a href=/tags/fluentd title=fluentd>fluentd</a>
<a href=/tags/kubernetes title=kubernetes>kubernetes</a>
<a href=/tags/mq title=mq>mq</a>
<a href=/tags/rpc title=rpc>rpc</a>
<a href=/tags/sofa title=sofa>sofa</a>
<a href=/tags/sofastack title=sofastack>sofastack</a>
<a href=/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6 title=中间件>中间件</a>
<a href=/tags/%E4%BA%91%E5%8E%9F%E7%94%9F title=云原生>云原生</a>
<a href=/tags/%E9%83%A8%E7%BD%B2 title=部署>部署</a></div></section><hr><h5>FRIENDS</h5><ul class=list-inline></ul></div></div></div></article><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:gcslyp@gmail.com><span class="fa-stack fa-lg"><i class="fa fa-circle fa-stack-2x"></i><i class="fa fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a href=https://twitter.com/lan31793328><span class="fa-stack fa-lg"><i class="fa fa-circle fa-stack-2x"></i><i class="fa fa-twitter fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/liangyuanpeng><span class="fa-stack fa-lg"><i class="fa fa-circle fa-stack-2x"></i><i class="fa fa-github fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; Hi,I`m lan , 2018<br><a href=https://themes.gohugo.io/hugo-theme-cleanwhite>CleanWhite Hugo Theme</a> by <a href=https://zhaohuabing.com>Huabing</a> |
<iframe style=margin-left:2px;margin-bottom:-5px frameborder=0 scrolling=0 width=100px height=20px src="https://ghbtns.com/github-btn.html?user=zhaohuabing&repo=hugo-theme-cleanwhite&type=star&count=true"></iframe></p></div></div></div></footer><script>function async(u,c){var d=document,t='script',o=d.createElement(t),s=d.getElementsByTagName(t)[0];o.src=u;if(c){o.addEventListener('load',function(e){c(null,e);},false);}
s.parentNode.insertBefore(o,s);}</script><script>if($('#tag_cloud').length!==0){async("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:'#bbbbee',end:'#0085a1'},};$('#tag_cloud a').tagcloud();})}</script><script>async("https://cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.js",function(){var $nav=document.querySelector("nav");if($nav)FastClick.attach($nav);})</script><script>var _baId='fad9c137f8ce239f9b323d36c871f8e6';var _hmt=_hmt||[];(function(){var hm=document.createElement("script");hm.src="//hm.baidu.com/hm.js?"+_baId;var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script></body></html>